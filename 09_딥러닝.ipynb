{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a = sess.run([train, hypot, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]]\n",
    "\n",
    "y_data = [0, 1, 1, 1, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=100)\n",
    "clf.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [[0, 0, 0], [1, 1, 1], [0, 1, 0], [1, 1, 0]]\n",
    "examples_label = [0, 0, 1, 1]\n",
    "\n",
    "result = clf.predict(examples)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = metrics.accuracy_score(examples_label, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 딥러닝을 이용한 XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a = sess.run([train, hypot2, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어의 수는 7개(deep), 입출력 연결의 수는 50개(wide)로 구현\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "# 세번째 레이어\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias3\")\n",
    "hypot3 = tf.sigmoid(tf.matmul(hypot2, W3) + b3)\n",
    "\n",
    "# 네번째 레이어\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight4\")\n",
    "b4 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias4\")\n",
    "hypot4 = tf.sigmoid(tf.matmul(hypot3, W4) + b4)\n",
    "\n",
    "# 다섯번째 레이어\n",
    "W5 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight5\")\n",
    "b5 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias5\")\n",
    "hypot5 = tf.sigmoid(tf.matmul(hypot4, W5) + b5)\n",
    "\n",
    "# 여섯번째 레이어\n",
    "W6 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight6\")\n",
    "b6 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias6\")\n",
    "hypot6 = tf.sigmoid(tf.matmul(hypot5, W6) + b6)\n",
    "\n",
    "# 일곱번째 레이어\n",
    "W7 = tf.Variable(tf.random_normal([50, 1]), tf.float32, name=\"weight7\")\n",
    "b7 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias7\")\n",
    "hypot7 = tf.sigmoid(tf.matmul(hypot6, W7) + b7)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot7) + (1 - y) * tf.log(1 - hypot7))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot7 > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot7, pred, accuracy], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.03892267]\n",
      " [0.9859158 ]\n",
      " [0.98493075]\n",
      " [0.96347225]\n",
      " [0.98320913]\n",
      " [0.9704084 ]\n",
      " [0.9527266 ]\n",
      " [0.12550944]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layer1\", hypot1)\n",
    "\n",
    "# 두번째 레이어\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"layer2\", hypot2)\n",
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a, summary = sess.run([train, hypot2, pred, accuracy, merged_summary], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate tf1\n",
    "# tensorboard --logdir=./log_dir2/alpha01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.77151823]\n",
      " [0.7696756 ]\n",
      " [0.7605102 ]\n",
      " [0.76514184]\n",
      " [0.77384305]\n",
      " [0.70003307]\n",
      " [0.7627694 ]\n",
      " [0.6944696 ]]\n",
      "예측 : [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 0.75\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.01\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], \n",
    "                 [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3]) \n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layer1\", hypot1)\n",
    "\n",
    "# 두번째 레이어\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"layer2\", hypot2)\n",
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha001\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a, summary = sess.run([train, hypot2, pred, accuracy, merged_summary], \n",
    "                              feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "        \n",
    "    print(\"가설 : {}\\n예측 : {}\\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate tf1\n",
    "# tensorboard --logdir=./log_dir2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU : Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-84fb6f4620c0>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002136ABCF5C8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002136CFD99C8>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002136ABFD348>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫번째 모델 구축 : 성능 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logit = tf.matmul(X, W) + b\n",
    "hypot = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(hypot, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 2.235273994965988\n",
      "epoch: 2     cost: 0.9391043697703962\n",
      "epoch: 3     cost: 0.7639257183941925\n",
      "epoch: 4     cost: 0.6727164927395903\n",
      "epoch: 5     cost: 0.6143821444294668\n",
      "epoch: 6     cost: 0.5720064725117251\n",
      "epoch: 7     cost: 0.5404931049997158\n",
      "epoch: 8     cost: 0.5163665371049532\n",
      "epoch: 9     cost: 0.49562065444209336\n",
      "epoch: 10     cost: 0.4778196140852845\n",
      "epoch: 11     cost: 0.4627257197553457\n",
      "epoch: 12     cost: 0.44977367612448577\n",
      "epoch: 13     cost: 0.43928282846104\n",
      "epoch: 14     cost: 0.42865928574041906\n",
      "epoch: 15     cost: 0.4200903660058977\n",
      "epoch: 16     cost: 0.41047290119257857\n",
      "epoch: 17     cost: 0.4031694471294226\n",
      "epoch: 18     cost: 0.39607837557792647\n",
      "epoch: 19     cost: 0.3912650289318776\n",
      "epoch: 20     cost: 0.3840723641352222\n",
      "epoch: 21     cost: 0.3792117125879636\n",
      "epoch: 22     cost: 0.37413775091821516\n",
      "epoch: 23     cost: 0.3697902725501496\n",
      "epoch: 24     cost: 0.36544411133636157\n",
      "epoch: 25     cost: 0.35982812052423274\n",
      "epoch: 26     cost: 0.35739627480506875\n",
      "epoch: 27     cost: 0.35265796758911816\n",
      "epoch: 28     cost: 0.35004447541453604\n",
      "epoch: 29     cost: 0.3469825650887059\n",
      "epoch: 30     cost: 0.3436968145587229\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9046\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이어 3개 추가, 입출력 갯수는 256개 : Relu 사용 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 3.036261453628538\n",
      "epoch: 2     cost: 1.3481488243016326\n",
      "epoch: 3     cost: 1.0905219463868574\n",
      "epoch: 4     cost: 0.9416170527718287\n",
      "epoch: 5     cost: 0.8340429847890678\n",
      "epoch: 6     cost: 0.8038948973742404\n",
      "epoch: 7     cost: 0.754783883094788\n",
      "epoch: 8     cost: 0.726126509796489\n",
      "epoch: 9     cost: 0.6532998365705666\n",
      "epoch: 10     cost: 0.6097514394196597\n",
      "epoch: 11     cost: 0.6101863527297975\n",
      "epoch: 12     cost: 0.6131344337896864\n",
      "epoch: 13     cost: 0.598084743781524\n",
      "epoch: 14     cost: 0.5829163516651504\n",
      "epoch: 15     cost: 0.5618706801804633\n",
      "epoch: 16     cost: 0.5538565170764921\n",
      "epoch: 17     cost: 0.5333796387368982\n",
      "epoch: 18     cost: 0.5208112226832996\n",
      "epoch: 19     cost: 0.5209048850969837\n",
      "epoch: 20     cost: 0.49973050258376384\n",
      "epoch: 21     cost: 0.48974530772729374\n",
      "epoch: 22     cost: 0.47323070818727647\n",
      "epoch: 23     cost: 0.47967440832744956\n",
      "epoch: 24     cost: 0.46601555705070463\n",
      "epoch: 25     cost: 0.44351017491383987\n",
      "epoch: 26     cost: 0.43713262357495036\n",
      "epoch: 27     cost: 0.46022426057945603\n",
      "epoch: 28     cost: 0.421554053317417\n",
      "epoch: 29     cost: 0.4076211742921309\n",
      "epoch: 30     cost: 0.42849890860644224\n",
      "훈련 종료\n",
      "정확도 :  0.8733\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit4, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xavier 초기화 : 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "epoch: 1     cost: 1.060418639291417\n",
      "epoch: 2     cost: 0.37780252299525535\n",
      "epoch: 3     cost: 0.307027799758044\n",
      "epoch: 4     cost: 0.26995997634800994\n",
      "epoch: 5     cost: 0.24262715261090884\n",
      "epoch: 6     cost: 0.2182717987895012\n",
      "epoch: 7     cost: 0.19961912339383925\n",
      "epoch: 8     cost: 0.18140536825765258\n",
      "epoch: 9     cost: 0.1647451191327789\n",
      "epoch: 10     cost: 0.15127315607937902\n",
      "epoch: 11     cost: 0.139346133037047\n",
      "epoch: 12     cost: 0.12850928114219146\n",
      "epoch: 13     cost: 0.11979239541021265\n",
      "epoch: 14     cost: 0.11132043182849884\n",
      "epoch: 15     cost: 0.10411816053769807\n",
      "epoch: 16     cost: 0.09737603282386602\n",
      "epoch: 17     cost: 0.09105099503966896\n",
      "epoch: 18     cost: 0.08602289540523829\n",
      "epoch: 19     cost: 0.08031059796159921\n",
      "epoch: 20     cost: 0.07674883759157217\n",
      "epoch: 21     cost: 0.07179031040858137\n",
      "epoch: 22     cost: 0.0680791825123809\n",
      "epoch: 23     cost: 0.06430984844538304\n",
      "epoch: 24     cost: 0.06066660169173373\n",
      "epoch: 25     cost: 0.05744754025881942\n",
      "epoch: 26     cost: 0.05393928416411984\n",
      "epoch: 27     cost: 0.05119891070845452\n",
      "epoch: 28     cost: 0.048552267578515135\n",
      "epoch: 29     cost: 0.0457717001438141\n",
      "epoch: 30     cost: 0.04330861313437872\n",
      "훈련 종료\n",
      "정확도 :  0.9734\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit4, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 좀 더 deep 하고 wide하게 : layer는 총 8개로 구성, 입출력 갯수는 512 : 97.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 1.8995259737968446\n",
      "epoch: 2     cost: 0.5053822723843835\n",
      "epoch: 3     cost: 0.3037702301957392\n",
      "epoch: 4     cost: 0.2202402510697193\n",
      "epoch: 5     cost: 0.17431518877094443\n",
      "epoch: 6     cost: 0.1427130840718745\n",
      "epoch: 7     cost: 0.12052008928223085\n",
      "epoch: 8     cost: 0.10416421607814048\n",
      "epoch: 9     cost: 0.09229872662912716\n",
      "epoch: 10     cost: 0.07854964004660193\n",
      "epoch: 11     cost: 0.066617853814228\n",
      "epoch: 12     cost: 0.0641036687114022\n",
      "epoch: 13     cost: 0.0534049643694677\n",
      "epoch: 14     cost: 0.047890528950162915\n",
      "epoch: 15     cost: 0.04171291448345235\n",
      "epoch: 16     cost: 0.5145011806504971\n",
      "epoch: 17     cost: 0.09662095935507256\n",
      "epoch: 18     cost: 0.09586566672406416\n",
      "epoch: 19     cost: 0.0464506482349878\n",
      "epoch: 20     cost: 0.03767310706221242\n",
      "epoch: 21     cost: 0.02751918024129487\n",
      "epoch: 22     cost: 0.022030701605941765\n",
      "epoch: 23     cost: 0.3314244840827516\n",
      "epoch: 24     cost: 0.049182196007194846\n",
      "epoch: 25     cost: 0.02807113849981266\n",
      "epoch: 26     cost: 0.020220932360653854\n",
      "epoch: 27     cost: 0.014910801273177966\n",
      "epoch: 28     cost: 0.014691391726320783\n",
      "epoch: 29     cost: 0.009269817486003213\n",
      "epoch: 30     cost: 0.006911652949215337\n",
      "훈련 종료\n",
      "정확도 :  0.9788\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout : 과적합 해결 방안 + AdamOptimizer : 98.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 0.9131051273237585\n",
      "epoch: 2     cost: 0.24223414765162898\n",
      "epoch: 3     cost: 0.17752889453010134\n",
      "epoch: 4     cost: 0.14671705182303083\n",
      "epoch: 5     cost: 0.126427684751424\n",
      "epoch: 6     cost: 0.1094750647450035\n",
      "epoch: 7     cost: 0.10226733932440929\n",
      "epoch: 8     cost: 0.08981828639791774\n",
      "epoch: 9     cost: 0.08561657074838883\n",
      "epoch: 10     cost: 0.0822009916434234\n",
      "epoch: 11     cost: 0.07059225179424337\n",
      "epoch: 12     cost: 0.07348698423328717\n",
      "epoch: 13     cost: 0.06459911588071421\n",
      "epoch: 14     cost: 0.06348817489871923\n",
      "epoch: 15     cost: 0.05930255390365017\n",
      "epoch: 16     cost: 0.06100901339118449\n",
      "epoch: 17     cost: 0.05750541071661494\n",
      "epoch: 18     cost: 0.05434733555652201\n",
      "epoch: 19     cost: 0.05582171615382486\n",
      "epoch: 20     cost: 0.05519720088859852\n",
      "epoch: 21     cost: 0.049812514697286224\n",
      "epoch: 22     cost: 0.04958065111867409\n",
      "epoch: 23     cost: 0.04500052633610642\n",
      "epoch: 24     cost: 0.044064258921214114\n",
      "epoch: 25     cost: 0.044616777874868015\n",
      "epoch: 26     cost: 0.041028836730190296\n",
      "epoch: 27     cost: 0.03901564252054824\n",
      "epoch: 28     cost: 0.04710919917053113\n",
      "epoch: 29     cost: 0.04753113202581351\n",
      "epoch: 30     cost: 0.044891557965258296\n",
      "훈련 종료\n",
      "정확도 :  0.9821\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "hypot1 = tf.nn.dropout(hypot1, keep_prob=prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "hypot2 = tf.nn.dropout(hypot2, keep_prob=prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "hypot3 = tf.nn.dropout(hypot3, keep_prob=prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "hypot4 = tf.nn.dropout(hypot4, keep_prob=prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "hypot5 = tf.nn.dropout(hypot5, keep_prob=prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "hypot6 = tf.nn.dropout(hypot6, keep_prob=prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "hypot7 = tf.nn.dropout(hypot7, keep_prob=prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "hypot8 = tf.nn.dropout(hypot8, keep_prob=prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys,\n",
    "                                                 prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels,\n",
    "                                             prob:1}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
