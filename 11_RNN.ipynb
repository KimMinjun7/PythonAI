{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단일 cell 을 통한 계산 처리 과정 : 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 준비\n",
    "inputs = np.array([[[1, 2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-4bb2c7a63d20>:9: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 3), dtype=float32)\n",
      "Tensor(\"rnn/while/Exit_3:0\", shape=(1, 3), dtype=float32)\n",
      "\n",
      " Weights\n",
      "<tf.Variable 'rnn/basic_rnn_cell/kernel:0' shape=(5, 3) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_rnn_cell/bias:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "\n",
    "# 1 * 3의 matrix 출력([6, 7, 8] 과 같은)\n",
    "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=3)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=rnn_cell, dtype=tf.float32, \n",
    "                                   inputs=tf_inputs)\n",
    "\n",
    "print(outputs)\n",
    "print(state)\n",
    "\n",
    "print(\"\\n Weights\")\n",
    "for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Value\n",
      "[[[-0.9314169   0.75578666 -0.6819246 ]]]\n",
      "State Value\n",
      "[[-0.9314169   0.75578666 -0.6819246 ]]\n",
      "['rnn/basic_rnn_cell/kernel:0', 'rnn/basic_rnn_cell/bias:0']\n",
      "rnn/basic_rnn_cell/kernel:0 [[-0.62831575  0.38538355  0.79733914]\n",
      " [-0.5203329   0.30046564 -0.8150209 ]\n",
      " [ 0.39399797  0.16670114  0.4062907 ]\n",
      " [-0.6391754   0.8460203   0.5266966 ]\n",
      " [ 0.41124135  0.66347724 -0.0210759 ]]\n",
      "rnn/basic_rnn_cell/bias:0 [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run, state_run = sess.run([outputs, state])\n",
    "    \n",
    "    print(\"Output Value\")\n",
    "    print(output_run)\n",
    "    \n",
    "    print(\"State Value\")\n",
    "    print(state_run)\n",
    "    \n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    print(variables_names)\n",
    "    values = sess.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(k, v)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다중 cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I work at google = [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]\n",
    "# I google at work = [[1,0,0,0], [0,0,0,1], [0,0,1,0], [0,1,0,0]]\n",
    "\n",
    "inputs = np.array([\n",
    "                [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]],\n",
    "                [[1,0,0,0], [0,0,0,1], [0,0,1,0], [0,1,0,0]]\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Tensor(\"rnn/transpose_1:0\", shape=(2, 4, 3), dtype=float32)\n",
      "--------------------------------------------------------------\n",
      "Tensor(\"rnn/while/Exit_3:0\", shape=(2, 3), dtype=float32)\n",
      "--------------------------------------------------------------\n",
      "<tf.Variable 'rnn/basic_rnn_cell/kernel:0' shape=(7, 3) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_rnn_cell/bias:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=3)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=rnn_cell, dtype=tf.float32, \n",
    "                                  inputs=tf_inputs)\n",
    "variables_names = [v.name for v in tf.trainable_variables()]\n",
    "\n",
    "print(outputs)\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(state)\n",
    "print(\"--------------------------------------------------------------\")\n",
    "for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Value\n",
      "[[[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.20793891  0.24406303 -0.75278705]\n",
      "  [-0.06346128 -0.52844936  0.68356085]\n",
      "  [-0.36491966  0.8857268  -0.02324395]]\n",
      "\n",
      " [[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.30707452  0.62735885  0.21719742]\n",
      "  [ 0.5043804  -0.14038289  0.3744523 ]\n",
      "  [-0.11641277  0.70696247 -0.7512605 ]]]\n",
      "State Value\n",
      "[[-0.36491966  0.8857268  -0.02324395]\n",
      " [-0.11641277  0.70696247 -0.7512605 ]]\n",
      "rnn/basic_rnn_cell/kernel:0 [[-0.56198275  0.34469748  0.7131618 ]\n",
      " [-0.4653999   0.2687447  -0.7289769 ]\n",
      " [ 0.35240245  0.14910203  0.36339748]\n",
      " [-0.57169586  0.7567036   0.47109187]\n",
      " [ 0.3678255   0.5934322  -0.01885086]\n",
      " [ 0.31208777 -0.40880746  0.22867584]\n",
      " [ 0.5521256   0.682691   -0.5481483 ]]\n",
      "rnn/basic_rnn_cell/bias:0 [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run, state_run = sess.run([outputs, state])\n",
    "    \n",
    "    print(\"Output Value\")\n",
    "    print(output_run)\n",
    "    \n",
    "    print(\"State Value\")\n",
    "    print(state_run)\n",
    "    \n",
    "    values = sess.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose_1:0\", shape=(1, 5, 3), dtype=float32)\n",
      "--------------------------------------------------------------\n",
      "Tensor(\"rnn/while/Exit_3:0\", shape=(1, 3), dtype=float32)\n",
      "--------------------------------------------------------------\n",
      "<tf.Variable 'rnn/basic_rnn_cell/kernel:0' shape=(7, 3) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_rnn_cell/bias:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=3)\n",
    "\n",
    "x_data = np.array([[h, e, l, l, o]], dtype=np.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=rnn_cell, dtype=tf.float32, \n",
    "                                  inputs=x_data)\n",
    "\n",
    "variables_names = [v.name for v in tf.trainable_variables()]\n",
    "\n",
    "print(outputs)\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(state)\n",
    "print(\"--------------------------------------------------------------\")\n",
    "for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Value\n",
      "[[[ 0.03521261 -0.55469215  0.3911671 ]\n",
      "  [ 0.71793234 -0.6416993  -0.23057865]\n",
      "  [ 0.28477573 -0.04065027 -0.6732457 ]\n",
      "  [ 0.27927202 -0.02790875 -0.29825467]\n",
      "  [ 0.5107646   0.44046617 -0.58083504]]]\n",
      "State Value\n",
      "[[ 0.5107646   0.44046617 -0.58083504]]\n",
      "rnn/basic_rnn_cell/kernel:0 [[ 0.03522718 -0.62513345  0.41317725]\n",
      " [ 0.7187083  -0.7458999   0.24596906]\n",
      " [ 0.07847679 -0.2686134  -0.3954072 ]\n",
      " [ 0.5364299   0.36027014 -0.70091194]\n",
      " [-0.4692109   0.02520639  0.03773957]\n",
      " [-0.691509   -0.20686752  0.75621474]\n",
      " [-0.4662865  -0.3343653  -0.16011602]]\n",
      "rnn/basic_rnn_cell/bias:0 [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run, state_run = sess.run([outputs, state])\n",
    "    \n",
    "    print(\"Output Value\")\n",
    "    print(output_run)\n",
    "    \n",
    "    print(\"State Value\")\n",
    "    print(state_run)\n",
    "    \n",
    "    values = sess.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hihello 학습 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = ['h','i','e','l','o']\n",
    "x_one_hot = [[[1,0,0,0,0],\n",
    "             [0,1,0,0,0],\n",
    "             [1,0,0,0,0],\n",
    "             [0,0,1,0,0],\n",
    "             [0,0,0,1,0],\n",
    "             [0,0,0,1,0],\n",
    "             [0,0,0,0,1]]]\n",
    "\n",
    "# x_data = [[0,1,0,2,3,3]] #hihell\n",
    "y_data = [[0,1,0,2,3,3,4]] #hihello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "sequence_len = 7 # x_one_hot의 행개수\n",
    "num_classes = 5\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# X, y 준비\n",
    "X = tf.placeholder(tf.float32, shape=[None, sequence_len, num_classes])\n",
    "y = tf.placeholder(tf.int32, shape=[None, sequence_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose_1:0\", shape=(?, 7, 5), dtype=float32)\n",
      "Tensor(\"rnn/while/Exit_3:0\", shape=(?, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# rnn 모델\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=num_classes)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32)\n",
    "print(outputs)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 평면화(Flat Layer)\n",
    "X_for_fc = tf.reshape(outputs, [-1, num_classes])\n",
    "print(X_for_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers\\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "0 cost: 1.5103203    prediction: [[0 1 0 3 3 3 3]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "1 cost: 1.1951942    prediction: [[0 1 0 2 3 3 4]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "2 cost: 0.92564857    prediction: [[0 1 0 2 3 3 4]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "3 cost: 0.6775916    prediction: [[0 1 0 2 3 3 4]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "4 cost: 0.49333224    prediction: [[0 1 0 2 3 3 4]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "5 cost: 0.35018376    prediction: [[0 1 0 2 3 3 4]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "6 cost: 0.25466552    prediction: [[0 1 0 2 3 3 4]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "7 cost: 0.18750243    prediction: [[0 1 0 2 3 3 4]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "8 cost: 0.13522802    prediction: [[0 1 0 2 3 3 4]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "9 cost: 0.09569024    prediction: [[0 1 0 2 3 3 4]]     정답: [[0, 1, 0, 2, 3, 3, 4]]\n",
      "결과 : h, i, h, e, l, l, o\n"
     ]
    }
   ],
   "source": [
    "# FC (W, b, logit)\n",
    "outputs = tf.contrib.layers.fully_connected(inputs=X_for_fc, num_outputs=num_classes, \n",
    "                                  activation_fn=None)\n",
    "\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_len, num_classes])\n",
    "W = tf.ones([batch_size, sequence_len])\n",
    "cost = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(logits=outputs, \n",
    "                                        targets=y, weights=W))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(10):\n",
    "        _, c = sess.run([train, cost], feed_dict={X:x_one_hot, y:y_data})\n",
    "        result = sess.run(tf.argmax(outputs, 2), feed_dict={X:x_one_hot})\n",
    "        print(i, 'cost:', c, \"   prediction:\", result,\"    정답:\",y_data)\n",
    "        \n",
    "    result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "    print('결과 :',', '.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 두 번째 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'n', 'i', 'a', 'f', ' ', 'w', 't', 'o', 'y']\n",
      "{'u': 0, 'n': 1, 'i': 2, 'a': 3, 'f': 4, ' ': 5, 'w': 6, 't': 7, 'o': 8, 'y': 9}\n",
      "[5, 2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]\n"
     ]
    }
   ],
   "source": [
    "#sample = \" You will have to submit to your fate whether you will or not.\"\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))\n",
    "print(idx2char)\n",
    "\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}\n",
    "print(char2idx)\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample]\n",
    "print(sample_idx)\n",
    "\n",
    "x_data = [sample_idx[:-1]]\n",
    "y_data = [sample_idx[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "num_classes = len(char2idx)\n",
    "batch_size = 1\n",
    "sequence_len = len(sample) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# X, y 준비\n",
    "X = tf.placeholder(tf.int32, shape=[None, sequence_len])\n",
    "y = tf.placeholder(tf.int32, shape=[None, sequence_len])\n",
    "\n",
    "x_one_hot = tf.one_hot(X, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=num_classes)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=x_one_hot, dtype=tf.float32)\n",
    "\n",
    "W = tf.ones([batch_size, sequence_len])\n",
    "cost = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(logits=outputs, \n",
    "                                                        targets=y, weights=W))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 2.3314052    prediction: [[6 2 2 9 5 2 5 0 9 2 6 6 9 5 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "1 cost: 2.0096936    prediction: [[9 8 5 9 5 0 5 0 9 5 1 6 9 5 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "2 cost: 1.8021482    prediction: [[9 8 5 9 5 0 5 6 9 5 7 5 9 5 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "3 cost: 1.6359936    prediction: [[9 8 5 9 8 0 5 6 9 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "4 cost: 1.5283053    prediction: [[9 8 5 9 8 0 5 6 8 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "5 cost: 1.4471759    prediction: [[9 8 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "6 cost: 1.3742923    prediction: [[9 8 5 9 8 0 5 6 8 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "7 cost: 1.3016006    prediction: [[9 8 5 9 8 0 5 6 8 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "8 cost: 1.2558782    prediction: [[9 8 5 9 8 0 5 6 8 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "9 cost: 1.1945007    prediction: [[9 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "10 cost: 1.1579311    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "11 cost: 1.1246051    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "12 cost: 1.0904847    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "13 cost: 1.0520498    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "14 cost: 1.0267005    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "15 cost: 1.0147742    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "16 cost: 0.9925925    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "17 cost: 0.97859424    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "18 cost: 0.9661618    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "19 cost: 0.95374846    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "20 cost: 0.94041306    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "21 cost: 0.9281277    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "22 cost: 0.9164523    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "23 cost: 0.9063764    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "24 cost: 0.89892423    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "25 cost: 0.8902112    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "26 cost: 0.8845159    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "27 cost: 0.8773684    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "28 cost: 0.87365496    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "29 cost: 0.8679454    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "30 cost: 0.86381334    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "31 cost: 0.86018443    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "32 cost: 0.8562345    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "33 cost: 0.85299927    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "34 cost: 0.8503014    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "35 cost: 0.84720737    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "36 cost: 0.84473735    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "37 cost: 0.8424736    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "38 cost: 0.8401201    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "39 cost: 0.8380367    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "40 cost: 0.83631605    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "41 cost: 0.8345681    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "42 cost: 0.8328902    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "43 cost: 0.83146447    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "44 cost: 0.83011025    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "45 cost: 0.8287775    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "46 cost: 0.8276384    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "47 cost: 0.82668245    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "48 cost: 0.8257118    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "49 cost: 0.8248538    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "50 cost: 0.8241292    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "51 cost: 0.82339543    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "52 cost: 0.8226818    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "53 cost: 0.8220695    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "54 cost: 0.821457    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "55 cost: 0.8208459    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "56 cost: 0.82031643    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "57 cost: 0.8198021    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "58 cost: 0.8192908    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "59 cost: 0.8188423    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "60 cost: 0.8184177    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "61 cost: 0.8179905    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "62 cost: 0.81761104    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "63 cost: 0.81725043    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "64 cost: 0.81688774    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "65 cost: 0.81655765    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "66 cost: 0.8162464    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "67 cost: 0.8159336    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "68 cost: 0.8156485    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 cost: 0.81537783    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "70 cost: 0.8151085    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "71 cost: 0.8148612    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "72 cost: 0.81462485    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "73 cost: 0.8143907    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "74 cost: 0.814175    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "75 cost: 0.81396645    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "76 cost: 0.8137617    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "77 cost: 0.813572    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "78 cost: 0.81338626    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "79 cost: 0.81320566    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "80 cost: 0.8130363    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "81 cost: 0.81286913    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "82 cost: 0.8127079    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "83 cost: 0.8125548    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "84 cost: 0.81240314    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "85 cost: 0.8122583    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "86 cost: 0.81211853    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "87 cost: 0.81198096    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "88 cost: 0.8118497    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "89 cost: 0.8117217    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "90 cost: 0.8115966    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "91 cost: 0.81147665    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "92 cost: 0.8113585    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "93 cost: 0.811244    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "94 cost: 0.811133    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "95 cost: 0.8110236    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "96 cost: 0.8109178    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "97 cost: 0.81081414    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "98 cost: 0.8107127    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "99 cost: 0.8106143    prediction: [[2 4 5 9 8 0 5 6 3 1 7 5 9 8 0]]     정답: [[2, 4, 5, 9, 8, 0, 5, 6, 3, 1, 7, 5, 9, 8, 0]]\n",
      "결과 : i, f,  , y, o, u,  , w, a, n, t,  , y, o, u\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(100):\n",
    "        _, c = sess.run([train, cost], feed_dict={X:x_data, y:y_data})\n",
    "        result = sess.run(tf.argmax(outputs, 2), feed_dict={X:x_data})\n",
    "        print(i, 'cost:', c, \"   prediction:\", result,\"    정답:\",y_data)\n",
    "        \n",
    "    result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "    print('결과 :',', '.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM(Long Short Term Memory)\n",
    "\n",
    "+ BackProgation Through TIme(BPTT)\n",
    "    - Gradient Vanishing\n",
    "    - Gradient Exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.09927537]]]\n",
      "[[0.09927537]]\n",
      "[[0.18134572]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "inputs = np.array([[[1,0]]])\n",
    "\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=1)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell=cell, dtype=tf.float32, inputs = tf_inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_run, states_run = sess.run([outputs, states])\n",
    "    \n",
    "    print(outputs_run)\n",
    "    print(states_run.h)\n",
    "    print(states_run.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the garlic fries were a great starter (and a h...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>our meal was excellent i had the pasta ai form...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what i enjoy most about palo alto is so many r...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the drinks came out fairly quickly a good two ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>despite the not so good burger the service was...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the four reigning major champions simona halep...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the briton was seeded nn7 here last year befor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stephens surged her way back from injury in st...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when it came to england chances in the world c...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the team that eliminated russia – croatia – al...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category\n",
       "0   dishplace is located in sunnyvale downtown the...     food\n",
       "1   service can be slower during busy hours but ou...     food\n",
       "2   portions are huge both french toast and their ...     food\n",
       "3   we started with apps going the chicken and waf...     food\n",
       "4   the biscuits and gravy was too salty two peopl...     food\n",
       "5   the garlic fries were a great starter (and a h...     food\n",
       "6   our meal was excellent i had the pasta ai form...     food\n",
       "7   what i enjoy most about palo alto is so many r...     food\n",
       "8   the drinks came out fairly quickly a good two ...     food\n",
       "9   despite the not so good burger the service was...     food\n",
       "10  the four reigning major champions simona halep...   sports\n",
       "11  the briton was seeded nn7 here last year befor...   sports\n",
       "12  stephens surged her way back from injury in st...   sports\n",
       "13  when it came to england chances in the world c...   sports\n",
       "14  the team that eliminated russia – croatia – al...   sports\n",
       "15  the perseyside outfit finished in fourth place...   sports\n",
       "16  liverpool fc will return to premier league act...   sports\n",
       "17  alisson signed for liverpool fc from as roma t...   sports\n",
       "18  but the rankings during that run-in to new yor...   sports\n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_dict_list = [\n",
    "         {'paragraph': 'dishplace is located in sunnyvale downtown there is parking around the area but it can be difficult to find during peak business hours my sisters and i came to this place for dinner on a weekday they were really busy so i highly recommended making reservations unless you have the patience to wait', 'category': 'food'},\n",
    "         {'paragraph': 'service can be slower during busy hours but our waiter was courteous and help gave some great entree recommendations', 'category': 'food'},\n",
    "         {'paragraph': 'portions are huge both french toast and their various omelettes are really good their french toast is probably 1.5x more than other brunch places great place to visit if you are hungry and dont want to wait 1 hour for a table', 'category': 'food'},\n",
    "         {'paragraph': 'we started with apps going the chicken and waffle slides and chicken nachos the sliders were amazing and the nachos were good too maybe by themselves the nachos would have scored better but after those sliders they were up against some tough competition', 'category': 'food'},\n",
    "         {'paragraph': 'the biscuits and gravy was too salty two people in my group had the gravy and all thought it was too salty my hubby ordered a side of double egg and it was served on two small plates who serves eggs to one person on separate plates we commented on that when it was delivered and even the server laughed and said she doesnt know why the kitchen does that presentation of food is important and they really missed on this one', 'category': 'food'},\n",
    "         {'paragraph': 'the garlic fries were a great starter (and a happy hour special) the pancakes looked and tasted great and were a fairly generous portion', 'category': 'food'},\n",
    "         {'paragraph': 'our meal was excellent i had the pasta ai formaggi which was so rich i didnt dare eat it all although i certainly wanted to excellent flavors with a great texture contrast between the soft pasta and the crisp bread crumbs too much sauce for me but a wonderful dish', 'category': 'food'},\n",
    "         {'paragraph': 'what i enjoy most about palo alto is so many restaurants have dog-friendly seating outside i had bookmarked italico from when they first opened about a 1.5 years ago and was jonesing for some pasta so time to finally knock that bookmark off', 'category': 'food'},\n",
    "         {'paragraph': 'the drinks came out fairly quickly a good two to three minutes after the orders were taken i expected my iced tea to taste a bit more sweet but this was straight up green tea with ice in it not to complain of course but i was pleasantly surprised', 'category': 'food'},\n",
    "         {'paragraph': 'despite the not so good burger the service was so slow the restaurant wasnt even half full and they took very long from the moment we got seated to the time we left it was almost 2 hours we thought that it would be quick since we ordered as soon as we sat down my coworkers did seem to enjoy their beef burgers for those who eat beef however i will not be returning it is too expensive and extremely slow service', 'category': 'food'},\n",
    "    \n",
    "         {'paragraph': 'the four reigning major champions simona halep caroline wozniacki angelique kerber and defending us open champion sloane stephens could make a case for being the quartet most likely to succeed especially as all but stephens has also enjoyed the no1 ranking within the last 14 months as they prepare for their gruelling new york campaigns they currently hold the top four places in the ranks', 'category': 'sports'},\n",
    "         {'paragraph': 'the briton was seeded nn7 here last year before a slump in form and confidence took her down to no46 after five first-round losses but there have been signs of a turnaround including a victory over a sub-par serena williams in san jose plus wins against jelena ostapenko and victoria azarenka in montreal. konta pulled out of new haven this week with illness but will hope for good things where she first scored wins in a major before her big breakthroughs to the semis in australia and wimbledon', 'category': 'sports'},\n",
    "         {'paragraph': 'stephens surged her way back from injury in stunning style to win her first major here last year—and ranked just no83 she has since proved what a big time player she is winning the miami title via four fellow major champions then reaching the final at the french open back on north american hard courts she ran to the final in montreal only just edged out by halep she has also avoided many of the big names in her quarter—except for wild card azarenka as a possible in the third round', 'category': 'sports'},\n",
    "         {'paragraph': 'when it came to england chances in the world cup it would be fair to say that most fans had never been more pessimistic than they were this year after enduring years of truly dismal performances at major tournaments – culminating in the 2014 event where they failed to win any of their three group games and finished in bottom spot those results led to the resignation of manager roy hodgson', 'category': 'sports'},\n",
    "         {'paragraph': 'the team that eliminated russia – croatia – also improved enormously during the tournament before it began their odds were 33/1 but they played with real flair and star players like luka modric ivan rakitic and ivan perisic showed their quality on the world stage having displayed their potential by winning all three of their group stage games croatia went on to face difficult tests like the semi-final against england', 'category': 'sports'},\n",
    "         {'paragraph': 'the perseyside outfit finished in fourth place in the premier league table and without a trophy last term after having reached the champions league final before losing to real madrid', 'category': 'sports'},\n",
    "         {'paragraph': 'liverpool fc will return to premier league action on saturday lunchtime when they travel to leicester city in the top flight as they look to make it four wins in a row in the league', 'category': 'sports'},\n",
    "         {'paragraph': 'alisson signed for liverpool fc from as roma this summer and the brazilian goalkeeper has helped the reds to keep three clean sheets in their first three premier league games', 'category': 'sports'},\n",
    "         {'paragraph': 'but the rankings during that run-in to new york hid some very different undercurrents for murray had struggled with a hip injury since the clay swing and had not played a match since losing his quarter-final at wimbledon and he would pull out of the us open just two days before the tournament began—too late however to promote nederer to the no2 seeding', 'category': 'sports'},\n",
    "         {'paragraph': 'then came the oh-so-familiar djokovic-nadal no-quarter-given battle for dominance in the thiadal more than once pulled off a reverse smash and had his chance to seal the tie-break but it was djokovic serving at 10-9 who dragged one decisive error from nadal for a two-sets lead', 'category': 'sports'}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(paragraph_dict_list)\n",
    "df = df[[\"paragraph\", \"category\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "19    None\n",
       "Name: paragraph, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = set()\n",
    "df[\"paragraph\"].str.lower().str.split().apply(results.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(and',\n",
       " '1',\n",
       " '1.5',\n",
       " '1.5x',\n",
       " '10-9',\n",
       " '14',\n",
       " '2',\n",
       " '2014',\n",
       " '33/1',\n",
       " 'a',\n",
       " 'about',\n",
       " 'action',\n",
       " 'after',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'ai',\n",
       " 'alisson',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'also',\n",
       " 'although',\n",
       " 'alto',\n",
       " 'amazing',\n",
       " 'american',\n",
       " 'and',\n",
       " 'angelique',\n",
       " 'any',\n",
       " 'apps',\n",
       " 'are',\n",
       " 'area',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'australia',\n",
       " 'avoided',\n",
       " 'azarenka',\n",
       " 'back',\n",
       " 'battle',\n",
       " 'be',\n",
       " 'beef',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'began—too',\n",
       " 'being',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'biscuits',\n",
       " 'bit',\n",
       " 'bookmark',\n",
       " 'bookmarked',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'brazilian',\n",
       " 'bread',\n",
       " 'breakthroughs',\n",
       " 'briton',\n",
       " 'brunch',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'campaigns',\n",
       " 'can',\n",
       " 'card',\n",
       " 'caroline',\n",
       " 'case',\n",
       " 'certainly',\n",
       " 'champion',\n",
       " 'champions',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'chicken',\n",
       " 'city',\n",
       " 'clay',\n",
       " 'clean',\n",
       " 'commented',\n",
       " 'competition',\n",
       " 'complain',\n",
       " 'confidence',\n",
       " 'contrast',\n",
       " 'could',\n",
       " 'course',\n",
       " 'courteous',\n",
       " 'courts',\n",
       " 'coworkers',\n",
       " 'crisp',\n",
       " 'croatia',\n",
       " 'crumbs',\n",
       " 'culminating',\n",
       " 'cup',\n",
       " 'currently',\n",
       " 'dare',\n",
       " 'days',\n",
       " 'decisive',\n",
       " 'defending',\n",
       " 'delivered',\n",
       " 'despite',\n",
       " 'did',\n",
       " 'didnt',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'dinner',\n",
       " 'dish',\n",
       " 'dishplace',\n",
       " 'dismal',\n",
       " 'displayed',\n",
       " 'djokovic',\n",
       " 'djokovic-nadal',\n",
       " 'does',\n",
       " 'doesnt',\n",
       " 'dog-friendly',\n",
       " 'dominance',\n",
       " 'dont',\n",
       " 'double',\n",
       " 'down',\n",
       " 'downtown',\n",
       " 'dragged',\n",
       " 'drinks',\n",
       " 'during',\n",
       " 'eat',\n",
       " 'edged',\n",
       " 'egg',\n",
       " 'eggs',\n",
       " 'eliminated',\n",
       " 'enduring',\n",
       " 'england',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enormously',\n",
       " 'entree',\n",
       " 'error',\n",
       " 'especially',\n",
       " 'even',\n",
       " 'event',\n",
       " 'excellent',\n",
       " 'expected',\n",
       " 'expensive',\n",
       " 'extremely',\n",
       " 'face',\n",
       " 'failed',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fans',\n",
       " 'fc',\n",
       " 'fellow',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'first-round',\n",
       " 'five',\n",
       " 'flair',\n",
       " 'flavors',\n",
       " 'flight',\n",
       " 'food',\n",
       " 'for',\n",
       " 'form',\n",
       " 'formaggi',\n",
       " 'four',\n",
       " 'fourth',\n",
       " 'french',\n",
       " 'fries',\n",
       " 'from',\n",
       " 'full',\n",
       " 'games',\n",
       " 'garlic',\n",
       " 'gave',\n",
       " 'generous',\n",
       " 'goalkeeper',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'gravy',\n",
       " 'great',\n",
       " 'green',\n",
       " 'group',\n",
       " 'gruelling',\n",
       " 'had',\n",
       " 'halep',\n",
       " 'half',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hid',\n",
       " 'highly',\n",
       " 'hip',\n",
       " 'his',\n",
       " 'hodgson',\n",
       " 'hold',\n",
       " 'hope',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'however',\n",
       " 'hubby',\n",
       " 'huge',\n",
       " 'hungry',\n",
       " 'i',\n",
       " 'ice',\n",
       " 'iced',\n",
       " 'if',\n",
       " 'illness',\n",
       " 'important',\n",
       " 'improved',\n",
       " 'in',\n",
       " 'including',\n",
       " 'injury',\n",
       " 'is',\n",
       " 'it',\n",
       " 'italico',\n",
       " 'ivan',\n",
       " 'jelena',\n",
       " 'jonesing',\n",
       " 'jose',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'kerber',\n",
       " 'kitchen',\n",
       " 'knock',\n",
       " 'know',\n",
       " 'konta',\n",
       " 'last',\n",
       " 'late',\n",
       " 'laughed',\n",
       " 'lead',\n",
       " 'league',\n",
       " 'led',\n",
       " 'left',\n",
       " 'leicester',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'liverpool',\n",
       " 'located',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'losing',\n",
       " 'losses',\n",
       " 'luka',\n",
       " 'lunchtime',\n",
       " 'madrid',\n",
       " 'major',\n",
       " 'make',\n",
       " 'making',\n",
       " 'manager',\n",
       " 'many',\n",
       " 'match',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'meal',\n",
       " 'miami',\n",
       " 'minutes',\n",
       " 'missed',\n",
       " 'modric',\n",
       " 'moment',\n",
       " 'months',\n",
       " 'montreal',\n",
       " 'montreal.',\n",
       " 'more',\n",
       " 'most',\n",
       " 'much',\n",
       " 'murray',\n",
       " 'my',\n",
       " 'nachos',\n",
       " 'nadal',\n",
       " 'names',\n",
       " 'nederer',\n",
       " 'never',\n",
       " 'new',\n",
       " 'nn7',\n",
       " 'no-quarter-given',\n",
       " 'no1',\n",
       " 'no2',\n",
       " 'no46',\n",
       " 'no83',\n",
       " 'north',\n",
       " 'not',\n",
       " 'odds',\n",
       " 'of',\n",
       " 'off',\n",
       " 'oh-so-familiar',\n",
       " 'omelettes',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'ordered',\n",
       " 'orders',\n",
       " 'ostapenko',\n",
       " 'other',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outfit',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'palo',\n",
       " 'pancakes',\n",
       " 'parking',\n",
       " 'pasta',\n",
       " 'patience',\n",
       " 'peak',\n",
       " 'people',\n",
       " 'performances',\n",
       " 'perisic',\n",
       " 'perseyside',\n",
       " 'person',\n",
       " 'pessimistic',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plates',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'pleasantly',\n",
       " 'plus',\n",
       " 'portion',\n",
       " 'portions',\n",
       " 'possible',\n",
       " 'potential',\n",
       " 'premier',\n",
       " 'prepare',\n",
       " 'presentation',\n",
       " 'probably',\n",
       " 'promote',\n",
       " 'proved',\n",
       " 'pull',\n",
       " 'pulled',\n",
       " 'quality',\n",
       " 'quarter-final',\n",
       " 'quarter—except',\n",
       " 'quartet',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'rakitic',\n",
       " 'ran',\n",
       " 'ranked',\n",
       " 'ranking',\n",
       " 'rankings',\n",
       " 'ranks',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'real',\n",
       " 'really',\n",
       " 'recommendations',\n",
       " 'recommended',\n",
       " 'reds',\n",
       " 'reigning',\n",
       " 'reservations',\n",
       " 'resignation',\n",
       " 'restaurant',\n",
       " 'restaurants',\n",
       " 'results',\n",
       " 'return',\n",
       " 'returning',\n",
       " 'reverse',\n",
       " 'rich',\n",
       " 'roma',\n",
       " 'round',\n",
       " 'row',\n",
       " 'roy',\n",
       " 'run-in',\n",
       " 'russia',\n",
       " 'said',\n",
       " 'salty',\n",
       " 'san',\n",
       " 'sat',\n",
       " 'saturday',\n",
       " 'sauce',\n",
       " 'say',\n",
       " 'scored',\n",
       " 'seal',\n",
       " 'seated',\n",
       " 'seating',\n",
       " 'seeded',\n",
       " 'seeding',\n",
       " 'seem',\n",
       " 'semi-final',\n",
       " 'semis',\n",
       " 'separate',\n",
       " 'serena',\n",
       " 'served',\n",
       " 'server',\n",
       " 'serves',\n",
       " 'service',\n",
       " 'serving',\n",
       " 'she',\n",
       " 'sheets',\n",
       " 'showed',\n",
       " 'side',\n",
       " 'signed',\n",
       " 'signs',\n",
       " 'simona',\n",
       " 'since',\n",
       " 'sisters',\n",
       " 'sliders',\n",
       " 'slides',\n",
       " 'sloane',\n",
       " 'slow',\n",
       " 'slower',\n",
       " 'slump',\n",
       " 'small',\n",
       " 'smash',\n",
       " 'so',\n",
       " 'soft',\n",
       " 'some',\n",
       " 'soon',\n",
       " 'special)',\n",
       " 'spot',\n",
       " 'stage',\n",
       " 'star',\n",
       " 'started',\n",
       " 'starter',\n",
       " 'stephens',\n",
       " 'straight',\n",
       " 'struggled',\n",
       " 'stunning',\n",
       " 'style',\n",
       " 'sub-par',\n",
       " 'succeed',\n",
       " 'summer',\n",
       " 'sunnyvale',\n",
       " 'surged',\n",
       " 'surprised',\n",
       " 'sweet',\n",
       " 'swing',\n",
       " 'table',\n",
       " 'taken',\n",
       " 'taste',\n",
       " 'tasted',\n",
       " 'tea',\n",
       " 'team',\n",
       " 'term',\n",
       " 'tests',\n",
       " 'texture',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'they',\n",
       " 'thiadal',\n",
       " 'things',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'tie-break',\n",
       " 'time',\n",
       " 'title',\n",
       " 'to',\n",
       " 'toast',\n",
       " 'too',\n",
       " 'took',\n",
       " 'top',\n",
       " 'tough',\n",
       " 'tournament',\n",
       " 'tournaments',\n",
       " 'travel',\n",
       " 'trophy',\n",
       " 'truly',\n",
       " 'turnaround',\n",
       " 'two',\n",
       " 'two-sets',\n",
       " 'undercurrents',\n",
       " 'unless',\n",
       " 'up',\n",
       " 'us',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'victoria',\n",
       " 'victory',\n",
       " 'visit',\n",
       " 'waffle',\n",
       " 'wait',\n",
       " 'waiter',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'was',\n",
       " 'wasnt',\n",
       " 'way',\n",
       " 'we',\n",
       " 'week',\n",
       " 'weekday',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'who',\n",
       " 'why',\n",
       " 'wild',\n",
       " 'will',\n",
       " 'williams',\n",
       " 'wimbledon',\n",
       " 'win',\n",
       " 'winning',\n",
       " 'wins',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonderful',\n",
       " 'world',\n",
       " 'would',\n",
       " 'wozniacki',\n",
       " 'year',\n",
       " 'years',\n",
       " 'year—and',\n",
       " 'york',\n",
       " 'you',\n",
       " '–'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\n",
      "alto\n"
     ]
    }
   ],
   "source": [
    "idx2word=dict(enumerate(results))\n",
    "print(idx2word[396])\n",
    "print(idx2word[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "word2idx={v:k for k, v in idx2word.items()}\n",
    "print(word2idx[\"results\"])\n",
    "print(word2idx[\"alto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_paragraph(para):\n",
    "    words = para.split()\n",
    "    encoded=[]\n",
    "    for w in words:\n",
    "        encoded.append([word2idx[w]])\n",
    "        \n",
    "    return encoded\n",
    "\n",
    "def encoded_category(cate):\n",
    "    if cate==\"food\":\n",
    "        return [1,0]\n",
    "    else:\n",
    "        return [0,1]\n",
    "    \n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[367], [152], [293], [276], [339], [165], [92...\n",
       "1     [[196], [262], [201], [428], [527], [229], [26...\n",
       "2     [[119], [283], [491], [191], [493], [392], [90...\n",
       "3     [[518], [497], [321], [121], [480], [274], [35...\n",
       "4     [[274], [125], [90], [269], [192], [360], [150...\n",
       "5     [[274], [143], [348], [2], [40], [304], [460],...\n",
       "6     [[232], [446], [192], [82], [441], [475], [274...\n",
       "7     [[219], [441], [13], [157], [473], [470], [100...\n",
       "8     [[274], [8], [15], [139], [130], [17], [40], [...\n",
       "9     [[68], [274], [317], [449], [5], [102], [274],...\n",
       "10    [[274], [112], [79], [50], [474], [172], [294]...\n",
       "11    [[274], [14], [192], [492], [291], [27], [251]...\n",
       "12    [[412], [259], [369], [489], [512], [421], [41...\n",
       "13    [[46], [231], [15], [387], [381], [189], [276]...\n",
       "14    [[274], [62], [242], [526], [420], [163], [397...\n",
       "15    [[274], [509], [331], [486], [276], [136], [10...\n",
       "16    [[256], [528], [213], [178], [387], [105], [89...\n",
       "17    [[206], [361], [500], [256], [528], [421], [50...\n",
       "18    [[33], [274], [299], [527], [242], [365], [387...\n",
       "19    [[225], [15], [274], [110], [30], [399], [173]...\n",
       "Name: paragraph, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.paragraph.apply(encoded_paragraph)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [1, 0]\n",
       "1     [1, 0]\n",
       "2     [1, 0]\n",
       "3     [1, 0]\n",
       "4     [1, 0]\n",
       "5     [1, 0]\n",
       "6     [1, 0]\n",
       "7     [1, 0]\n",
       "8     [1, 0]\n",
       "9     [1, 0]\n",
       "10    [0, 1]\n",
       "11    [0, 1]\n",
       "12    [0, 1]\n",
       "13    [0, 1]\n",
       "14    [0, 1]\n",
       "15    [0, 1]\n",
       "16    [0, 1]\n",
       "17    [0, 1]\n",
       "18    [0, 1]\n",
       "19    [0, 1]\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.apply(encoded_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"seq_length\"] = df.paragraph.apply(word_cnt)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "# 문장의 최대 길이 알아내기\n",
    "max_word_cnt = 0\n",
    "\n",
    "for row in df[\"paragraph\"]:\n",
    "    if len(row.split())> max_word_cnt:\n",
    "        max_word_cnt = len(row.split())\n",
    "        \n",
    "print(max_word_cnt)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 길이가 91개가 안될 경우 -1로 채워준다\n",
    "def sequence_padding(enc_para):\n",
    "    seq_len = len(enc_para)\n",
    "    for i in range(seq_len, max_word_cnt):\n",
    "        enc_para.append([-1])\n",
    "        \n",
    "    return enc_para\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'enc_paragraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-1b95829c2ba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"enc_paragraph\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_paragraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_padding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5138\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'enc_paragraph'"
     ]
    }
   ],
   "source": [
    "df[\"enc_paragraph\"] = df.enc_paragraph.apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'enc_paragraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'enc_paragraph'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-7ccc489f4de2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"enc_paragraph\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'enc_paragraph'"
     ]
    }
   ],
   "source": [
    "print(df[\"enc_paragraph\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'enc_paragraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-726a564a8286>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 입력값들을 배열로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menc_paragraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_paragraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0menc_category\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5138\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'enc_paragraph'"
     ]
    }
   ],
   "source": [
    "# 입력값들을 배열로 변환\n",
    "enc_paragraph = np.array(df.enc_paragraph.tolist())\n",
    "enc_category = np.array(df.cnc.category.tolist())\n",
    "seq_length = np.array(df.seq_length.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_paragraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-06e7cbce68e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_paragraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_category\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'enc_paragraph' is not defined"
     ]
    }
   ],
   "source": [
    "train_X = enc_paragraph\n",
    "train_y = enc_category\n",
    "\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-cfee57145997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m output, state = tf.nn.dynamic_rnn(cell=cell, input=X, dtype=tf.float32,\n\u001b[1;32m---> 12\u001b[1;33m                                  sequence_length=seq_length)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seq_length' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "#parameter\n",
    "lr = 0.001\n",
    "n_epochs = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, max_word_cnt,1])\n",
    "y = tf.placeholder(tf.int32, [None, 2])\n",
    "\n",
    "embeded = tf.layer.dense(X,5)\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64)\n",
    "output, state = tf.nn.dynamic_rnn(cell=cell, input=X, dtype=tf.float32,\n",
    "                                 sequence_length=seq_length)\n",
    "\n",
    "#print(output)\n",
    "#print(state)\n",
    "\n",
    "first_layer = tf.layers.dense(state,h,32)\n",
    "logit = tf.layers.dense(first_layer, 2)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, lab))\n",
    "train = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "\n",
    "#print(X)\n",
    "#print(state)\n",
    "#print(first_layer)\n",
    "#print(logit)\n",
    "#print(embeded)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        _,c = sess.run([train,cost], feed_dict={X:train_X, y:train_y})\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            pred=tf.nn.softmax(logit)\n",
    "            correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "            accuracy=tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "            cur_acc = accuracy.eval({X:train_X, y:train_y})\n",
    "            print(\"epoch:\"+str(epoch)+ \", cost:\" +str(cost)+ \", acc:\" +str(cur_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://insightcampus.co.kr/insightcommunity/?mod=document&uid=12936\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "\n",
    "# Target log path\n",
    "logs_path = 'log_dir/tmp/tensorflow/rnn_words'\n",
    "writer = tf.summary.FileWriter(logs_path)\n",
    "\n",
    "# Text file containing words for training\n",
    "training_file = 'data/belling_the_cat.txt'\n",
    "\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
    "    content = np.array(content)\n",
    "    \n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Loaded training data...\")\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "\n",
    "    # 2-layer LSTM, each layer has n_hidden units.\n",
    "    # Average Accuracy= 95.20% at 50k iter\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "    # Average Accuracy= 90.60% 50k iter\n",
    "    # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above\n",
    "    # rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < training_iters:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = training_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "    print(\"Run on command line.\")\n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"Point your web browser to: http://localhost:6006/\")\n",
    "    while True:\n",
    "        prompt = \"%s words: \" % n_input\n",
    "        sentence = input(prompt)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        if len(words) != n_input:\n",
    "            continue\n",
    "        try:\n",
    "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(32):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "            print(sentence)\n",
    "        except:\n",
    "            print(\"Word not in dictionary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
